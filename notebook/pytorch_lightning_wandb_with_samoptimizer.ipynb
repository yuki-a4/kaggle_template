{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('exp0000', 'ccb91580f89c', '51100d20-c24f-4c69-b39a-8179a4dcb846')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "import uuid\n",
    "\n",
    "EXP_ID = str(uuid.uuid4())\n",
    "PROJECT = \"siim-isic-melanoma-classification\"\n",
    "NB = \"exp0000\"\n",
    "DESCRIPTION = \"test notebook\"\n",
    "HOST = socket.gethostname()\n",
    "NB, HOST, EXP_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
    "import gc\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import h5py  # HDF5のライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedGroupKFold, StratifiedKFold\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(\"../\")\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "OUTPUT_DIR = ROOT_DIR / \"output\"\n",
    "CP_DIR = OUTPUT_DIR / \"checkpoint\"\n",
    "\n",
    "\n",
    "def to_pickle(filename, obj):\n",
    "    with open(filename, mode=\"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def unpickle(filename):\n",
    "    with open(filename, mode=\"rb\") as fo:\n",
    "        p = pickle.load(fo)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path(\"../\")\n",
    "DATA_DIR = ROOT_DIR / Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(\"cpu count:\", multiprocessing.cpu_count())\n",
    "\n",
    "\n",
    "class Config:\n",
    "    N_LABEL = 10\n",
    "    N_FOLD = 5\n",
    "    RANDOM_SATE = 42\n",
    "    LR = 1.0e-05\n",
    "    MAX_LR = 8.0e-5\n",
    "    PATIENCE = 6\n",
    "    EPOCH = 10\n",
    "    BATCH_SIZE = 288\n",
    "    SKIP_EVALUATE_NUM = 0\n",
    "    BACK_BONE = \"tf_efficientnet_b1_ns\"\n",
    "    RUN_FOLD_COUNT = 10\n",
    "    IMG_SIZE = 139\n",
    "    T_MAX = 20\n",
    "    ETA_MIN = 3.0e-7\n",
    "    SCHEDULER_GAMMA = 1.0\n",
    "    ACCUMULATION_STEMP = 2\n",
    "    NUM_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(seed=Config.RANDOM_SATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(Config.RANDOM_SATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_csv(DATA_DIR / \"train-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select([pl.col(name).n_unique().alias(name) for name in train_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_csv(DATA_DIR / \"train-metadata.csv\")\n",
    "test_df = pl.read_csv(DATA_DIR / \"test-metadata.csv\")\n",
    "\n",
    "display(train_df.shape)\n",
    "display(train_df.head(3))\n",
    "display(test_df.shape)\n",
    "display(test_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.filter(pl.col(\"target\") == 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.with_columns(\n",
    "    pl.col(\"age_approx\").cast(pl.String).replace(\"NA\", np.nan).cast(pl.Float64),\n",
    ")\n",
    "\n",
    "test_df = test_df.with_columns(\n",
    "    pl.col(\"age_approx\").cast(pl.String).replace(\"NA\", np.nan).cast(pl.Float64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 1e-5\n",
    "\n",
    "num_cols = [\n",
    "    'age_approx',                        # Approximate age of patient at time of imaging.\n",
    "    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n",
    "    'tbp_lv_A',                          # A inside  lesion.+\n",
    "    'tbp_lv_Aext',                       # A outside lesion.+\n",
    "    'tbp_lv_B',                          # B inside  lesion.+\n",
    "    'tbp_lv_Bext',                       # B outside lesion.+ \n",
    "    'tbp_lv_C',                          # Chroma inside  lesion.+\n",
    "    'tbp_lv_Cext',                       # Chroma outside lesion.+\n",
    "    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n",
    "    'tbp_lv_Hext',                       # Hue outside lesion.+\n",
    "    'tbp_lv_L',                          # L inside lesion.+\n",
    "    'tbp_lv_Lext',                       # L outside lesion.+\n",
    "    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n",
    "    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n",
    "    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n",
    "    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaLB',                    #\n",
    "    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n",
    "    'tbp_lv_eccentricity',               # Eccentricity.+\n",
    "    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n",
    "    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n",
    "    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n",
    "    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n",
    "    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n",
    "    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n",
    "    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n",
    "    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n",
    "    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n",
    "    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n",
    "    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n",
    "]\n",
    "\n",
    "new_num_cols = [\n",
    "    'lesion_size_ratio',             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n",
    "    'lesion_shape_index',            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n",
    "    'hue_contrast',                  # tbp_lv_H                - tbp_lv_Hext              abs\n",
    "    'luminance_contrast',            # tbp_lv_L                - tbp_lv_Lext              abs\n",
    "    'lesion_color_difference',       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt  \n",
    "    'border_complexity',             # tbp_lv_norm_border      + tbp_lv_symm_2axis\n",
    "    'color_uniformity',              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n",
    "\n",
    "    'position_distance_3d',          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n",
    "    'perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n",
    "    'area_to_perimeter_ratio',       # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n",
    "    'lesion_visibility_score',       # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n",
    "    'symmetry_border_consistency',   # tbp_lv_symm_2axis       * tbp_lv_norm_border\n",
    "    'consistency_symmetry_border',   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n",
    "\n",
    "    'color_consistency',             # tbp_lv_stdL             / tbp_lv_Lext\n",
    "    'consistency_color',             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n",
    "    'size_age_interaction',          # clin_size_long_diam_mm  * age_approx\n",
    "    'hue_color_std_interaction',     # tbp_lv_H                * tbp_lv_color_std_mean\n",
    "    'lesion_severity_index',         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n",
    "    'shape_complexity_index',        # border_complexity       + lesion_shape_index\n",
    "    'color_contrast_index',          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n",
    "\n",
    "    'log_lesion_area',               # tbp_lv_areaMM2          + 1  np.log\n",
    "    'normalized_lesion_size',        # clin_size_long_diam_mm  / age_approx\n",
    "    'mean_hue_difference',           # tbp_lv_H                + tbp_lv_Hext    / 2\n",
    "    'std_dev_contrast',              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n",
    "    'color_shape_composite_index',   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n",
    "    'lesion_orientation_3d',         # tbp_lv_y                , tbp_lv_x  np.arctan2\n",
    "    'overall_color_difference',      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n",
    "\n",
    "    'symmetry_perimeter_interaction',# tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n",
    "    'comprehensive_lesion_index',    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n",
    "    'color_variance_ratio',          # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n",
    "    'border_color_interaction',      # tbp_lv_norm_border      * tbp_lv_norm_color\n",
    "    'border_color_interaction_2',\n",
    "    'size_color_contrast_ratio',     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n",
    "    'age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx\n",
    "    'age_normalized_nevi_confidence_2',\n",
    "    'color_asymmetry_index',         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n",
    "\n",
    "    'volume_approximation_3d',       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n",
    "    'color_range',                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n",
    "    'shape_color_consistency',       # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n",
    "    'border_length_ratio',           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n",
    "    'age_size_symmetry_index',       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n",
    "    'index_age_size_symmetry',       # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.with_columns(\n",
    "        pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n",
    "    ).with_columns(\n",
    "        pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()), # You may want to impute test data with train\n",
    "    ).with_columns(\n",
    "        lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n",
    "        lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n",
    "        hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n",
    "        luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n",
    "        lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n",
    "        border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n",
    "        color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + err),\n",
    "    ).with_columns(\n",
    "        position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n",
    "        perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n",
    "        area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n",
    "        lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n",
    "        combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n",
    "        symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n",
    "        consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n",
    "    ).with_columns(\n",
    "        color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n",
    "        consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n",
    "        size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n",
    "        hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n",
    "        lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n",
    "        shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n",
    "        color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n",
    "    ).with_columns(\n",
    "        log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n",
    "        normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n",
    "        mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n",
    "        std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n",
    "        color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n",
    "        lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n",
    "        overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n",
    "    ).with_columns(\n",
    "        symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n",
    "        comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n",
    "        color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n",
    "        border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n",
    "        border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n",
    "        size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n",
    "        age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n",
    "        age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n",
    "        color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n",
    "    ).with_columns(\n",
    "        volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n",
    "        color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n",
    "        shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n",
    "        border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n",
    "        age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n",
    "        index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n",
    "    ).with_columns(\n",
    "            ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + err)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n",
    "    ).with_columns(\n",
    "        count_per_patient = pl.col('isic_id').count().over('patient_id'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.with_columns(\n",
    "        pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n",
    "    ).with_columns(\n",
    "        pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()), # You may want to impute test data with train\n",
    "    ).with_columns(\n",
    "        lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n",
    "        lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n",
    "        hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n",
    "        luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n",
    "        lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n",
    "        border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n",
    "        color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + err),\n",
    "    ).with_columns(\n",
    "        position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n",
    "        perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n",
    "        area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n",
    "        lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n",
    "        combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n",
    "        symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n",
    "        consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n",
    "    ).with_columns(\n",
    "        color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n",
    "        consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n",
    "        size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n",
    "        hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n",
    "        lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n",
    "        shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n",
    "        color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n",
    "    ).with_columns(\n",
    "        log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n",
    "        normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n",
    "        mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n",
    "        std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n",
    "        color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n",
    "        lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n",
    "        overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n",
    "    ).with_columns(\n",
    "        symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n",
    "        comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n",
    "        color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n",
    "        border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n",
    "        border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n",
    "        size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n",
    "        age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n",
    "        age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n",
    "        color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n",
    "    ).with_columns(\n",
    "        volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n",
    "        color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n",
    "        shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n",
    "        border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n",
    "        age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n",
    "        index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n",
    "    ).with_columns(\n",
    "            ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + err)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n",
    "    ).with_columns(\n",
    "        count_per_patient = pl.col('isic_id').count().over('patient_id'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COL1 = ['age_approx_patient_norm',  'tbp_lv_H_patient_norm', 'tbp_lv_H', 'clin_size_long_diam_mm', 'count_per_patient', 'tbp_lv_y_patient_norm', 'normalized_lesion_size', 'position_distance_3d_patient_norm', 'color_uniformity_patient_norm', 'tbp_lv_Hext', 'lesion_visibility_score_patient_norm', 'age_normalized_nevi_confidence_2']\n",
    "USE_COL2 = ['tbp_lv_H', 'clin_size_long_diam_mm', 'tbp_lv_perimeterMM', 'tbp_lv_minorAxisMM', 'tbp_lv_areaMM2']\n",
    "USE_COL = list(set(USE_COL1) | set(USE_COL2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select(pl.col(USE_COL).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select(pl.col(USE_COL).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#, 'mean_hue_difference_patient_norm', 'mean_hue_difference'\n",
    "train_df[USE_COL].to_pandas().corr().style.background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df.select(USE_COL))\n",
    "\n",
    "train_df = train_df.with_columns(\n",
    "    pl.DataFrame(scaler.transform(train_df.select(USE_COL)), schema=USE_COL)\n",
    ")\n",
    "\n",
    "test_df = test_df.with_columns(\n",
    "    pl.DataFrame(scaler.transform(test_df.select(USE_COL)), schema=USE_COL)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.with_columns(\n",
    "    pl.col(USE_COL).fill_nan(0)\n",
    ")\n",
    "\n",
    "test_df = test_df.with_columns(\n",
    "    pl.col(USE_COL).fill_nan(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select(pl.col(USE_COL).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil2cv(image):\n",
    "    \"\"\"PIL型 -> OpenCV型\"\"\"\n",
    "    new_image = np.array(image, dtype=np.uint8)\n",
    "    if new_image.ndim == 2:  # モノクロ\n",
    "        pass\n",
    "    elif new_image.shape[2] == 3:  # カラー\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "    elif new_image.shape[2] == 4:  # 透過\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def read_images_from_hdf5(file_path):\n",
    "    with h5py.File(file_path, \"r\") as file:\n",
    "        ids_list = list(file.keys())\n",
    "        ids_images = {}\n",
    "        for img_id in tqdm(ids_list):\n",
    "            image_data = file[img_id][()]\n",
    "            image = pil2cv(Image.open(io.BytesIO(image_data)))\n",
    "            ids_images[img_id] = np.array(image)\n",
    "\n",
    "    return ids_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_df.iter_rows():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''img_map = {}\n",
    "img_map = read_images_from_hdf5(DATA_DIR / \"train-image.hdf5\")\n",
    "\n",
    "test_img_map = read_images_from_hdf5(DATA_DIR / \"test-image.hdf5\")\n",
    "img_map.update(test_img_map)\n",
    "'''\n",
    "img_map = unpickle(\"../data/img_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(img_map.values())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    train_transform = [\n",
    "        albu.Transpose(p=0.5),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        #albu.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.3, p=0.5),\n",
    "        albu.RandomBrightnessContrast (brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n",
    "        # albu.LongestMaxSize(max_size=Config.IMG_SIZE, always_apply=False),\n",
    "        albu.RandomResizedCrop(\n",
    "            p=0.5, scale=[0.8, 1.0], height=Config.IMG_SIZE, width=Config.IMG_SIZE\n",
    "        ),\n",
    "        albu.ShiftScaleRotate(\n",
    "            p=0.5,\n",
    "            shift_limit=0.2,\n",
    "            scale_limit=0.2,\n",
    "            rotate_limit=90,\n",
    "            border_mode=0,\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "        ),\n",
    "        # albu.Cutout(num_holes=3, max_h_size=20, max_w_size=20, fill_value=0, p=0.5),\n",
    "        albu.Resize(height=Config.IMG_SIZE, width=Config.IMG_SIZE),\n",
    "        # albu.PadIfNeeded(always_apply=True, min_height=Config.IMG_SIZE, min_width=Config.IMG_SIZE, border_mode=2),\n",
    "        #albu.Cutout(max_h_size=int(Config.IMG_SIZE * 0.375), max_w_size=int(Config.IMG_SIZE * 0.375), num_holes=1, p=0.7), \n",
    "        albu.CoarseDropout(max_height=int(Config.IMG_SIZE * 0.175), max_width=int(Config.IMG_SIZE * 0.175), max_holes=1, p=0.4),\n",
    "        albu.Normalize(),\n",
    "        # albu.ToSepia(p=0.3),\n",
    "        # albu.ToGray(p=1)\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_test_augmentation():\n",
    "    train_transform = [\n",
    "        # albu.HorizontalFlip (p=0.5),\n",
    "        # albu.VerticalFlip(p=0.3),\n",
    "        # albu.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, p=0.5),\n",
    "        # albu.LongestMaxSize(max_size=Config.IMG_SIZE, always_apply=False),\n",
    "        albu.Resize(height=Config.IMG_SIZE, width=Config.IMG_SIZE),\n",
    "        # albu.PadIfNeeded(always_apply=True, min_height=Config.IMG_SIZE, min_width=Config.IMG_SIZE, border_mode=2),\n",
    "        # albu.HorizontalFlip(p=0.4),\n",
    "        albu.Normalize(),\n",
    "        # albu.VerticalFlip(p=0.3),\n",
    "        # albu.ShiftScaleRotate(p=0.3, shift_limit=0.2, scale_limit=0.2, rotate_limit=1, border_mode=0, value=0, mask_value=0),\n",
    "        # albu.RandomResizedCrop(p=0.5, scale=[0.9, 1.0], height=Config.IMG_SIZE, width=Config.IMG_SIZE),\n",
    "        # albu.ToSepia(p=0.3),\n",
    "        # albu.ToGray(p=1)\n",
    "    ]\n",
    "    return albu.Compose(train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (\n",
    "                    (torch.pow(p, 2) if group[\"adaptive\"] else 1.0)\n",
    "                    * p.grad\n",
    "                    * scale.to(p)\n",
    "                )\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert (\n",
    "            closure is not None\n",
    "        ), \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(\n",
    "            closure\n",
    "        )  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][\n",
    "            0\n",
    "        ].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "            torch.stack(\n",
    "                [\n",
    "                    ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad)\n",
    "                    .norm(p=2)\n",
    "                    .to(shared_device)\n",
    "                    for group in self.param_groups\n",
    "                    for p in group[\"params\"]\n",
    "                    if p.grad is not None\n",
    "                ]\n",
    "            ),\n",
    "            p=2,\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(true, pred):\n",
    "    y_hat = pred\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "    v_gt = abs(true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_hat])\n",
    "    # v_pred = 1.0 - y_hat\n",
    "\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (\n",
    "        partial_auc_scaled - 0.5\n",
    "    )\n",
    "\n",
    "    return partial_auc\n",
    "\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "def evaluation_torch(true, pred):\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = 1 - min_tpr\n",
    "\n",
    "    # AUROCメトリクスの初期化\n",
    "    auroc = BinaryAUROC(max_fpr=max_fpr)\n",
    "\n",
    "    # メトリクスをデバイスに移動\n",
    "    auroc = auroc.to(device)\n",
    "\n",
    "    # 1 - trueと1 - predを使用してAUROCを計算\n",
    "    v_gt = torch.abs(true - 1)\n",
    "    v_pred = 1.0 - pred\n",
    "\n",
    "    partial_auc = auroc(v_pred, v_gt.int())\n",
    "\n",
    "    # スケーリングされた部分的なAUCを計算\n",
    "    partial_auc_scaled = (partial_auc - 0.5 * max_fpr) / (1 - max_fpr)\n",
    "\n",
    "    # 最終的な部分的なAUCを計算\n",
    "    final_partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (\n",
    "        partial_auc_scaled - 0.5\n",
    "    )\n",
    "\n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, TQDMProgressBar, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "class CVDataSet(Dataset):\n",
    "    COLUMN_NAME = USE_COL\n",
    "    \n",
    "    def __init__(self, df, images, transforms, data_type=None):\n",
    "        self.df = df\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "        self.data_type = data_type\n",
    "        self.target = df[\"target\"].to_numpy()\n",
    "        self.features = df.select(self.COLUMN_NAME).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # target\n",
    "        if self.data_type in [\"train\", \"valid\"]:\n",
    "            target = self.target[idx]\n",
    "        else:\n",
    "            target = -1\n",
    "\n",
    "        # image\n",
    "        img = img_map[self.df[\"isic_id\"][idx]]\n",
    "\n",
    "        augmented = self.transforms(image=img)\n",
    "        img = augmented[\"image\"]\n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "\n",
    "        # tabular\n",
    "        x = self.features[idx]\n",
    "        \n",
    "        return img, x, target\n",
    "\n",
    "class CVNet(nn.Module):\n",
    "    def __init__(self, num_features=len(CVDataSet.COLUMN_NAME)):\n",
    "        super(CVNet, self).__init__()\n",
    "\n",
    "        self.base_model = timm.create_model(\n",
    "            Config.BACK_BONE, num_classes=0, pretrained=True, in_chans=3\n",
    "        )\n",
    "        base_model_features = self.base_model.num_features\n",
    "\n",
    "        num_features_all = int(num_features * 3) + base_model_features\n",
    "\n",
    "        self.img_layer = nn.Sequential(\n",
    "            nn.Linear(num_features, int(num_features * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(int(num_features * 2), int(num_features * 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "        )\n",
    "        \n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(num_features_all, int(num_features_all / 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(int(num_features_all / 2), int(num_features_all / 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(int(num_features_all / 4), int(num_features_all / 8)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(int(num_features_all / 8), 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, img):\n",
    "        img_out = self.base_model(img)\n",
    "\n",
    "        x = self.img_layer(x)\n",
    "\n",
    "        x = torch.cat([img_out, x], 1)\n",
    "        x = self.cls(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# define the LightningModule\n",
    "class CVModule(LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super(CVModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.validation_step_outputs = []\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization = False # SAM Optimizerを使うので\n",
    "\n",
    "    def forward(self, x, img):\n",
    "        return self.model(x, img)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        target, pred, loss_1, pauc = self._get_preds_loss_metric(batch)\n",
    "\n",
    "        # SAM Optimizerを使うのでbackward, optimizer.step, optimizer.zero_grad, scheduler.stepを手動でやる。\n",
    "        optimizer = self.optimizers()\n",
    "        self.manual_backward(loss_1)\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        target, pred, loss_2, pauc = self._get_preds_loss_metric(batch)\n",
    "        self.manual_backward(loss_2)\n",
    "\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        self.lr_schedulers().step()\n",
    "        \n",
    "        self.log(\"train_step_loss_first_step\", loss_1)\n",
    "        self.log(\"train_step_loss\", loss_2)\n",
    "        self.log(\"train_step_metric\", pauc)\n",
    "        return loss_1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        base_optimizer = optim.AdamW\n",
    "        optimizer = SAM(\n",
    "            self.model.parameters(), base_optimizer, lr=Config.LR, weight_decay=1.0e-02\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            max_lr=Config.MAX_LR,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"cos\",\n",
    "            div_factor=1.0e3,\n",
    "            final_div_factor=1.0e3,\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n",
    "\n",
    "    def validation_step(self, batch, batch_index):            \n",
    "        target, pred, loss, pauc = self._get_preds_loss_metric(batch)\n",
    "        pred = pred.sigmoid()\n",
    "        self.log(\"val_step_loss\", loss)\n",
    "        self.log(\"val_step_metric\", pauc)\n",
    "        self.validation_step_outputs.append(\n",
    "            {\"val_loss\": loss, \"pred\": pred, \"y\": target}\n",
    "        )\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        if trainer.global_step == 0:\n",
    "            wandb.define_metric(\"val_metric\", summary=\"max\")\n",
    "            \n",
    "        outputs = self.validation_step_outputs\n",
    "        pred = torch.hstack([x[\"pred\"] for x in outputs])\n",
    "        y = torch.hstack([x[\"y\"] for x in outputs])\n",
    "        pred_np = pred.to(\"cpu\").detach().numpy().copy()\n",
    "        y = y.to(\"cpu\").detach().numpy().copy()\n",
    "        pauc = evaluation(y, pred_np)\n",
    "        pauc_torch = torch.tensor([pauc])\n",
    "\n",
    "        print(\n",
    "            f\"val_metric - {self.current_epoch}: metric:{pauc:.6f}\"\n",
    "        )\n",
    "        self.log(\"val_metric\", pauc_torch)\n",
    "\n",
    "    # metricも計算すべき\n",
    "    def _get_preds_loss_metric(self, batch):\n",
    "        \"\"\"train/valid/test ステップが似ているための便利な関数\"\"\"\n",
    "        img, x, target = batch\n",
    "        img, x, target = img.float(),  x.float(), target.float()\n",
    "        pred = self.model(x, img).squeeze()\n",
    "        loss = self.criterion(pred, target)\n",
    "        metrics_value = evaluation_torch(target, pred)\n",
    "        return target, pred, loss, metrics_value\n",
    "\n",
    "# データローダーの設定\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, train_loader, valid_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "skf = StratifiedGroupKFold(\n",
    "    n_splits=Config.N_FOLD, random_state=Config.RANDOM_SATE, shuffle=True\n",
    ")\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(\n",
    "    skf.split(train_df, train_df[\"target\"], groups=train_df[\"patient_id\"])\n",
    "):\n",
    "    print(f\"====== {fold} ======\")\n",
    "\n",
    "    train, valid = train_df[train_index], train_df[test_index]\n",
    "\n",
    "    train_dataset = CVDataSet(train, img_map, get_augmentation(), data_type=\"train\")\n",
    "    valid_dataset = CVDataSet(\n",
    "        valid, img_map, get_test_augmentation(), data_type=\"valid\"\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "    data_module = DataModule(train_loader=train_dataloader, valid_loader=valid_dataloader)\n",
    "    model = CVModule(CVNet())\n",
    "\n",
    "    # モデルの訓練\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"../output/checkpoint\", # wandbLogger使っていると機能しない？？\n",
    "        # filename=f'{NB}-{fold}-{{epoch}}-{{val_metric:.2f}}',\n",
    "        filename=f\"{NB}-{fold}\",\n",
    "        monitor = \"val_metric\",\n",
    "        save_top_k=1,\n",
    "        mode=\"max\",\n",
    "        save_last=False,\n",
    "        enable_version_counter=False # ファイル名が重複する場合v0とかをつけない。上書く。\n",
    "    )\n",
    "\n",
    "    earystopping = EarlyStopping(\n",
    "        monitor=\"val_metric\",\n",
    "        patience=Config.PATIENCE,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor()\n",
    "    \n",
    "    wandb_logger = WandbLogger(project=PROJECT, group=f\"{NB}_{EXP_ID}\", name=f\"fold-{fold}\", save_dir=\"../output\")\n",
    "\n",
    "    # モデルの訓練\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        max_epochs=Config.EPOCH,\n",
    "        check_val_every_n_epoch=1,\n",
    "        num_sanity_val_steps=0,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks=[lr_monitor, earystopping, checkpoint_callback],\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    del (\n",
    "        model,\n",
    "        data_module,\n",
    "        trainer,\n",
    "        valid_dataloader,\n",
    "        train_dataloader,\n",
    "        train_dataset,\n",
    "        valid_dataset,\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "oof = np.zeros(len(train_df))\n",
    "cv_scores = {}\n",
    "skf = StratifiedGroupKFold(\n",
    "    n_splits=Config.N_FOLD, random_state=Config.RANDOM_SATE, shuffle=True\n",
    ")\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(\n",
    "    skf.split(train_df, train_df[\"target\"], groups=train_df[\"patient_id\"])\n",
    "):\n",
    "    print(f\"====== {fold} ======\")\n",
    "\n",
    "    valid = train_df[test_index]\n",
    "    valid_target = valid[\"target\"].to_numpy()\n",
    "\n",
    "    # TODO DataLoaderはDataModuleに色々まかせたい\n",
    "    valid_dataset = CVDataSet(\n",
    "        valid, img_map, get_test_augmentation(), data_type=\"valid\"\n",
    "    )\n",
    "    validloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "    module = CVModule(CVNet())\n",
    "    module.load_state_dict(\n",
    "        torch.load(f\"../output/checkpoint/{NB}-{fold}.ckpt\")[\"state_dict\"]\n",
    "    )\n",
    "    module.to(device).eval()\n",
    "\n",
    "    preds = []\n",
    "    n_iter_val = len(validloader)\n",
    "    for i, (img, x, target) in tqdm(enumerate(validloader), total=n_iter_val):\n",
    "        with torch.no_grad():\n",
    "            img, x, target = (\n",
    "                img.to(device).float(),\n",
    "                x.to(device).float(),\n",
    "                target.to(device).float(),\n",
    "            )\n",
    "            outputs = module(x, img).squeeze().sigmoid()\n",
    "            outputs_np = outputs.to(\"cpu\").detach().numpy().copy()\n",
    "            preds.append(outputs_np)\n",
    "\n",
    "    pauc = evaluation(valid_target, np.hstack(preds))\n",
    "    print(pauc)\n",
    "\n",
    "    oof[test_index] = np.hstack(preds).reshape(-1)\n",
    "    cv_scores[f\"cv{fold}\"] = pauc\n",
    "\n",
    "    del module, valid_dataset, validloader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
